{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation With Neural Networks\n",
    "\n",
    "*Isaac Mauro*\n",
    "\n",
    "For this project, I wanted to explore training a neural network on english text, and using it to generate new text. I was inspired to explore this topic after stumbling upon reddit user /u/deimorz subreddit, /r/SubredditSimulator. The subreddit only allows a collection of bots he has created to post content and comment, and the posts/comments are generated with a markov model trained with comments and posts from a bunch of different subreddits. It makes for some really interesting, and often hilarious results.\n",
    "\n",
    "I have decided to do something similar, except I will be using a recurrent neural network, and I will be training it with various texts, mainly Shakesphere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "#### The Data\n",
    "\n",
    "I am going to be using selections of Shakespeare as training data for my model. I have decided to use a character level model instead of a word or sentance level model, as it will be the best at generalizing the data (hopefully). The data is going to be treated as sequence data, and the model will try to make a prediction about what the next character will be in a sequence based on some number of previous characters. A text file with N characters, and a chosen sequence length D, will be split up into N-D sequences, each with D entries. This will form the X matrix. The Y matrix (target matrix) will also have N-D entries, but each entry only contains the character that follows the corresnponding sequence in X. The sequence length can be adjusted to tune how far back the model can remember, with a higher number increasing computation time but also allowing the model to make better predictions based on context further in the past. Increasing the sequence length will also proabbly require more hidden units per layer to effectively keep track of all the features in the data. \n",
    "\n",
    "#### The Network\n",
    "\n",
    "I have decided that to get the most out of this exercise, with the time I have, it would be best for me to use a high-level machine learning library as opposed to creating my own from scratch in numpy or TensorFlow. I settled on Keras as my library of choice. Keras is a high-level machine learning library for python that uses TensorFlow for the backend computations, and works out of the box with minimal code. It's API is simple and intuitive. The main driver for a keras ML implementation is called a model, and we will be using the Sequential model. Once a Sequential model is instantiated, you can add layers to the model, which are responsible for the end to end computation within the network. Keras allows you to add any number of layers to a model with model.add(), and each layer will pass output to the next automatically, with the correct output dimensions. You only have to specify input dimensions for the first layer added, and keras takes care of the rest. After the layers are all defined, you just have to compile the model with model.compile(), which allows you to choose a loss function, an optimization algorithm, and any metrics that you want to collect during training. This makes for a powerful but easy to use interface for building custom networks.\n",
    "\n",
    "The LSTM (Long Short-Term memory) layer in keras is what I will be using for our network. I'm not very confident in the full math behind LSTM networks, but I will try my best to explain the basic idea.\n",
    "\n",
    "LSTM networks are a solution to a problem that traditional recurrent neural networks have: not being able to \"remember\" features in the data that may be relevant, but are too far back in time from the current context. RNN's work very well if the predictions the network is making are based on more recent features, but they begin to deteriorate when they need to take into account features from further in the past. \n",
    "\n",
    "To solve this problem, LSTM networks use \"memory units\" which are more complex versions of the neurons used in regular RNN's. Each memory unit consists of the current state, an input gate, output gate, and forget gate. The input gate decides when the neuron will accept data for updates. The forget gate decides when the neuron will \"forget\" its current state, and replace it with a completely new state. Finally, the output gate decides when the neuron will \"fire\" and output something to the next layer. This combination of gates allows the neurons to hold onto information and only update it when the context calls for it, and it also allows a neuron to completely forget its current state and take on new functionality as the context changes. \n",
    "\n",
    "I also want to speak briefly about the other two keras layers I use in my networks. The first is a dropout layer. Dropout is a technique used to help with overfitting issues that many neural networks have. Dropout randomly turns off a percentage of neurons during each training iteration. This creates a number of \"thinned\" networks (subsets of the whole network) that are all trained with a small amount of data. At test time, none of neurons are turned off, and all of the weights get scaled. This forces the neurons to be less co-dependent with other neurons in the network during training, making the network better at generalizing during test time. Nitish Srivastava et al. wrote a short paper on the effectiveness of dropout, and it is linked in the references section.\n",
    "\n",
    "The other layer I use in my model is the Dense layer. A dense layer is a standard densely connected neural network that sums the inputs times the weights and then passes that to the activation function. I am using this layer as my output layer, with the softmax activation function. The softmax activation function outputs a probability matrix that is the same shape as the Y matrix. Each entry in the output represents the probability of a certain character being the next character in the sequence. Using this as the output layer, as opposed to a single output, allows the network to output probabilities of characters instead of trying to predict the next character exactly. This opens up the potential to adjust the \"temperature\" of the predictions. Changing the temperature can make the network make riskier decisions instead of the selecting the character with the highest probability every time. You could make it so the generation function (discussed in the next section) would randomly select a character from the 5,10,etc highest probabilities in the output. I chose to simply pick the character with the highest probability in my generation function, but I designed the network in this way so I could play with different temperatures after I get the network trained on a large data set. \n",
    "\n",
    "#### Preparing Data and Generating Text\n",
    "\n",
    "I started by creating a method to help prepare text files so it will be easy to train my net on different inputs. Notice at the end of this method I split the data into X and Y matrices, which represent the input sequence(X) and \"true\" output (Y). You can change the length of the sequences returned by adjusting seq_len. A longer sequence length will increase the ability of the network to remember further in the past, but it will increase computation time, and a larger network will be required to get meaningful results.\n",
    "\n",
    "After a model has been trained on the data, I have to generate new text with it. To do this, I created another function to make it easier. The model expects the same input dimensions for predictions as it had while training, so for each character we want to generate, it needs a sequence of characters. Because of this, the model needs a seed sequence to get it started. I chose to pull a random sample from the dataset as my seed, but the function allows you to input any seed you would like, as long as it matches the sequence length the model expects. The function iterates for as many times as a character needs to be generated. During each iteration, the model is fed in the input sequence, a new character is generated and appended to the result string and the input. After the predicted character is appended to the input, it is too long to be used in the next iteration, so chop off the first character and keep going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "def prepare_data(filename, verbose=False, seq_len=100):\n",
    "    '''\n",
    "    prepare_data helps prepare a text file for use within a neural network.\n",
    "    This function prints some information about the data, and creates dictionaries\n",
    "    to help convert the characters in the text file to numerical values so the \n",
    "    NN can use them effectively.\n",
    "    \n",
    "    Args: \n",
    "    filename: location of ASCII file to read\n",
    "    verbose: print extra information about the data\n",
    "    seq_len: length of the sequences to prepare\n",
    "    \n",
    "    Return:\n",
    "    char-to-int, int-to-char: dictionaries for converting chars to ints and vice versa\n",
    "    chars: list of unique characters in the dataset\n",
    "    X: matrix with all the input sequences\n",
    "    Y: matrix with all the outputs corresponding to input sequence\n",
    "    '''\n",
    "    data = open(filename, 'r').read()\n",
    "    chars = list(set(data))\n",
    "    char_to_int = {char:ix for ix, char in enumerate(chars)}\n",
    "    int_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    if verbose==True:\n",
    "        print(\"length of data:\", len(data), \"number unique chars:\", len(chars))\n",
    "        print(\"unique chars:\")\n",
    "        print(chars)\n",
    "    # Prepare the sequences for input into our RNN\n",
    "    # Note that X will not contain the first <seq_len> characters\n",
    "    dataX = [] # Input sequences (len(data)/seq_len X seq_len matrix)\n",
    "    dataY = [] # Ouput sequence (len(data) X 1 matrix)\n",
    "    for i in range(0, len(data) - seq_len, 1):\n",
    "        seq_in = data[i:i + seq_len]\n",
    "        seq_out = data[i + seq_len]\n",
    "        # make sure to convert the characters to ints\n",
    "        dataX.append([char_to_int[char] for char in seq_in]) \n",
    "        dataY.append(char_to_int[seq_out])\n",
    "    X = np.array(dataX)\n",
    "    Y = np.array(dataY)\n",
    "    print(\"X shape:\",X.shape)\n",
    "    print(\"Y shape:\",Y.shape)\n",
    "    return X, Y, char_to_int, int_to_char, len(chars)\n",
    "\n",
    "def generate(model, seed, vocab_len, int_to_char, length=100):\n",
    "    '''\n",
    "    This function generates characters based on the model and seed that it is given.\n",
    "    \n",
    "    args\n",
    "    model: compiled keras model\n",
    "    length: number of characters to generate, default 100\n",
    "    seed: len 100 sequence to get the generation started\n",
    "    vocab_len: length of text vocab\n",
    "    int-to-char,char_to_int: dictionaries for converting characters\n",
    "    \n",
    "    returns a string of generated characters\n",
    "    '''\n",
    "    result = \"\"\n",
    "    for i in range(length):\n",
    "        # reshape the seed into the input the dimensions our model expects\n",
    "        X = np.reshape(seed, (1, seed.shape[0], 1))\n",
    "        # normalize\n",
    "        X = X/float(vocab_len)\n",
    "        # Make a prediction\n",
    "        predict = model.predict(X)\n",
    "        # Pick the value with the highest probability and convert it to a character, and add it to the result\n",
    "        index = np.argmax(predict)\n",
    "        char = int_to_char[index]\n",
    "        result = result + char\n",
    "        # Append the prediction to the seed, and then remove the first element of the seed.\n",
    "        # This shifts the \"window\" over so our model can make a new prediction, also using the\n",
    "        # new character we just generated. This continues for as long as we have more characters to generate\n",
    "        seed = np.append(seed, index)\n",
    "        seed = np.delete(seed, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 174126 number unique chars: 85\n",
      "unique chars:\n",
      "['E', '\\ufeff', 'D', 'B', 'T', 'O', 'H', 'C', '?', 'm', '2', ':', 'F', 'Y', '(', ']', 'q', 'b', '4', 'v', 'o', 'N', '>', 'W', 'd', 't', '$', 'z', 'j', '3', '6', 'G', ',', 'P', 'Q', 'A', 'g', '9', ')', 'p', '*', ';', 'k', '8', 'f', 'h', 'i', '1', '7', '%', 'I', '<', 'u', \"'\", '.', 'J', 'S', 'U', 'R', '5', ' ', '!', 's', 'M', 'K', 'V', 'l', 'r', '/', '0', '\"', 'L', '@', 'x', 'w', '[', '-', 'n', 'Z', 'y', '\\n', 'a', 'c', 'X', 'e']\n",
      "X shape: (174026, 100)\n",
      "Y shape: (174026,)\n"
     ]
    }
   ],
   "source": [
    "dataX,dataY,char_to_int,int_to_char,vocab_len = prepare_data('randjfull.txt',verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "The documentation calls for the input matrix to be of the shape [number samples, size of samples, number outputs]. This means we need to reshape our X matrix, which is currently of the shape [number samples, size of samples]. \n",
    "\n",
    "We also need to normalize the inputs, as is standard for most ML tasks.\n",
    "\n",
    "Finally, we need to convert the outputs (Y matrix) to a one-hot encoding. A one-hot encoding is a sparse array with a length equal to the number of unique characters, and each index refers to a one of those characters. So each entry in T will be a sparse array of all zeros, except for the index that refers to the character, which will be a 1. This allows me to use the softmax output layer and have the loss function be able to compare Y with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape X to follow keras.layers.LSTM specification\n",
    "X = np.reshape(dataX, (dataX.shape[0], dataX.shape[1], 1))\n",
    "# Normalize X so all the data falls with in range(0,1). Nice and easy because all our inputs are > 0\n",
    "X = X/float(vocab_len)\n",
    "# Do the one-hot encoding of Y. keras provides a very simple way to do this\n",
    "Y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building up our network. I want to begin with a smaller network to see how well it works, and then move onto larger networks.\n",
    "\n",
    "The first layer is our LSTM hidden layer, with 100 \"memory units\".\n",
    "\n",
    "Finally, we can compile the neural network, which sets everything up on the TensorFlow backend automatically. It requires that you specify a loss function and an optimizer function. In class, we have pretty much only dealt with mean-squared error or classification error as loss functions. For this network, I will be using a categorical cross-entropy loss function, which is supposed to work better for classification problems with lots of classes (we have 85 \"classes\" in this particular dataset). For our optimization function we will use *adam* which is similar to the gradient descent we used in class, but it is more efficient, especially when the data is large or has a lot of parameters. Using this instead of regular gradient descent should make training much faster. Keras makes it really easy to customize every bit of our network, and we could have even defined our own loss and optimization functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializes the model. Can also do this whole block as a one-liner by\n",
    "# using a list of the layer definitions as an argument for the Sequential constructor\n",
    "model1 = Sequential()\n",
    "# First layer. The first layer in a model requires the input shape be specified\n",
    "model1.add(LSTM(100, input_shape = (X.shape[1],X.shape[2])))\n",
    "# Second layer\n",
    "model1.add(Dense(Y.shape[1], activation='softmax'))\n",
    "# Compile the network to the TensorFlow backend\n",
    "model1.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the computation time required for training these networks, keras provides utilities to create checkpoints during training that save the network configuration and all of the weights at each checkpoint every time there is an improvement in loss after an iteration (epoch). This allows us to leave the network to train and come back to see which iteration has given the best loss, and then use the models state during that iteration to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each checkpoint will create a file with a name that indicates the epoch and loss\n",
    "file=\"model1-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# The checkpoints will be made each time the loss improves between epoch's \n",
    "checkpoint = ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# This is what we pass into the training function\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that checkpoints are setup, we can fit the model. model.fit() allows us to specify a list of *callback functions* which will be ran after every iteration. In our case, the callback list only contains the checkpoint function which creates a model checkpoint everytime the criteria described above is met. I have set the number of epoch's to a low number for the initial fit to keep training time to a minimum. The batch size indicates how many samples to train with at a time. With a batch size of 100, 100*100 characters will be loaded at a time, which should ensure that I have enough memory during each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.fit(X, Y, epochs = 20, batch_size = 100, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''ff'X:SZ'f5''ff'Z:SZ'f5d'uffS:uZ'C5Z:So:dF@ZB'Z:d'ZqSo:dF@ZZ'Zw:''ff':qZZ'Zw:'if51:uZ'C5Z:So:dF@ZB'Z:d'ZqSo:dF@ZB'Zw:'iff1:uZ'C5Z:So:dF@ZB'Z:d'ZqSo:dF@ZZ''f''yd:''ffS:y''f''fB:f'Zf''ydd:'Zf''f5''ff':\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the best loss from the checkpoints\n",
    "model1.load_weights('model1-improvement-19-2.4693.hdf5')\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random sequence from the data\n",
    "rand = np.random.randint(0, dataX.shape[0]-1)\n",
    "seed = dataX[rand]\n",
    "result = generate(model1, seed, vocab_len, int_to_char, length=200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our network is both overfitting to the data, and definitely is not big enough, nor trained on enough iterations to give meaningful results. So its time to try a new network, this time including multiple LSTM layers and a dropout layer.\n",
    "\n",
    "Note: I accidentally reset the cell aboves output when editing this journal before submission. The model was trained on a text file that I accidently deleted. The new Romeo and Juliet I downloaded had different characters, and the dictionaries were created differently when preparing the data with the new text file, so the output has all the wrong characters, making it completely nonsensical. You can still see the repeating patterns though, which was much more apparent when it was english text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "# First layer, this time with more units\n",
    "model2.add(LSTM(250, input_shape = (X.shape[1],X.shape[2]), return_sequences=True))\n",
    "# Dropout for the first layer\n",
    "model2.add(Dropout(0.2))\n",
    "# Second layer\n",
    "model2.add(LSTM(250))\n",
    "# Dropout for second layer\n",
    "model2.add(Dropout(0.2))\n",
    "# Output layer\n",
    "model2.add(Dense(Y.shape[1], activation='softmax'))\n",
    "# Compile the network to the TensorFlow backend\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "#Setup checkpoints\n",
    "file=\"model2-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#Train, this time with more epochs \n",
    "model2.fit(X, Y, epochs = 50, batch_size = 60, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up stopping this training early after it ran for 14 hours, the change in loss had started to slow down and I was impatient. Lets see if it is working a bit better. I'll also try some of the saved weights for the earlier epochs to see what our model has learned over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to beat the ware to b\n"
     ]
    }
   ],
   "source": [
    "# Load a model from first epoch\n",
    "model2.load_weights('model2-improvement-01-2.6089.hdf5')\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "rand = np.random.randint(0, dataX.shape[0]-1)\n",
    "seed = dataX[rand]\n",
    "result = generate(model2, seed, vocab_len, int_to_char, length=200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ao the eoen\n",
      "    The here in the farth ahe seart ahe seart and the siarn.\n",
      "    The  our ho teart and the  ousen the eoen\n",
      "    The here in the farth ahe seart ahe seart and the siarn.\n",
      "    The  our ho tea\n"
     ]
    }
   ],
   "source": [
    "# Load a model from 5th epoch\n",
    "model2.load_weights('model2-improvement-05-2.1107.hdf5')\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "rand = np.random.randint(0, dataX.shape[0]-1)\n",
    "seed = dataX[rand]\n",
    "result = generate(model2, seed, vocab_len, int_to_char, length=200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be the fest ahou srt ahould be the fack.\n",
      "\n",
      "  Rom. What say you teres and the world will be brow the sin.\n",
      "\n",
      "  Jul. I would the word and the world with ahe sends.\n",
      "\n",
      "  Rom. What say you tears and the  hildren of the sound.\n",
      "    The forgring to be oour mere to my love.\n",
      "    The forering to be oour to be to \n"
     ]
    }
   ],
   "source": [
    "# Load the model from the best epoch\n",
    "model2.load_weights('model2-improvement-23-1.6399.hdf5')\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random sequence from the data\n",
    "rand = np.random.randint(0, dataX.shape[0]-1)\n",
    "seed = dataX[rand]\n",
    "result = generate(model2, seed, vocab_len, int_to_char, length=300)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the samples above, the model is definitely learning, but I think it could do with some more tuning. A larger network will hopefully help, and I also want to decrease the sequence size a bit so I can train it faster. I am going to up the units in each layer to 512. I'm also going to increase the dropout a bit because multiple runs with different random seeds tend to produce similar patterns, indicating a potential overfit to the data. I also have changed to a slightly smaller dataset, still shakesphere, but only 25 selected sonnets, making for a dataset about 1/10 of the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 16889 number unique chars: 67\n",
      "unique chars:\n",
      "['D', 'O', 'H', 'B', 'T', '?', 'C', 'm', 'F', ':', '2', 'q', 'Y', '4', '(', 'b', 'v', 'o', 'N', 'W', 'd', 't', 'z', '3', '6', 'j', 'G', 'P', 'A', 'g', ')', '9', 'p', 'c', ';', 'k', '8', 'f', 'i', 'h', '1', '7', 'I', 'u', \"'\", '.', 'S', 'U', 'R', '5', ' ', 's', 'M', 'V', 'l', 'r', '0', 'L', 'x', 'w', '-', 'n', 'y', '\\n', 'a', ',', 'e']\n",
      "X shape: (16839, 50)\n",
      "Y shape: (16839,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data again, this time with a shorter sequence length\n",
    "dataX2,dataY2,char_to_int,int_to_char,vocab_len = prepare_data('sonnets.txt',verbose=True, seq_len=50)\n",
    "# Reshape X to follow keras.layers.LSTM specification\n",
    "X = np.reshape(dataX2, (dataX2.shape[0], dataX2.shape[1], 1))\n",
    "# Normalize X \n",
    "X = X/float(vocab_len)\n",
    "# Do the one-hot encoding of Y.\n",
    "Y = np_utils.to_categorical(dataY2)\n",
    "# Setup the 3 layer model\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(512, input_shape=(X.shape[1],X.shape[2]), return_sequences=True))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(LSTM(512))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(vocab_len, activation='softmax'))\n",
    "model3.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "#Setup checkpoints\n",
    "file=\"model3-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 3.0376Epoch 00000: loss improved from inf to 3.03785, saving model to model3-improvement-00-3.0379.hdf5\n",
      "16839/16839 [==============================] - 339s - loss: 3.0379   \n",
      "Epoch 2/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 3.0091Epoch 00001: loss improved from 3.03785 to 3.00933, saving model to model3-improvement-01-3.0093.hdf5\n",
      "16839/16839 [==============================] - 353s - loss: 3.0093   \n",
      "Epoch 3/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.9944Epoch 00002: loss improved from 3.00933 to 2.99349, saving model to model3-improvement-02-2.9935.hdf5\n",
      "16839/16839 [==============================] - 333s - loss: 2.9935   \n",
      "Epoch 4/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.9566Epoch 00003: loss improved from 2.99349 to 2.95544, saving model to model3-improvement-03-2.9554.hdf5\n",
      "16839/16839 [==============================] - 323s - loss: 2.9554   \n",
      "Epoch 5/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.8760Epoch 00004: loss improved from 2.95544 to 2.87629, saving model to model3-improvement-04-2.8763.hdf5\n",
      "16839/16839 [==============================] - 322s - loss: 2.8763   \n",
      "Epoch 6/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.8059Epoch 00005: loss improved from 2.87629 to 2.80567, saving model to model3-improvement-05-2.8057.hdf5\n",
      "16839/16839 [==============================] - 324s - loss: 2.8057   \n",
      "Epoch 7/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.7622Epoch 00006: loss improved from 2.80567 to 2.76230, saving model to model3-improvement-06-2.7623.hdf5\n",
      "16839/16839 [==============================] - 318s - loss: 2.7623   \n",
      "Epoch 8/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.7069Epoch 00007: loss improved from 2.76230 to 2.70738, saving model to model3-improvement-07-2.7074.hdf5\n",
      "16839/16839 [==============================] - 316s - loss: 2.7074   \n",
      "Epoch 9/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.6596Epoch 00008: loss improved from 2.70738 to 2.65950, saving model to model3-improvement-08-2.6595.hdf5\n",
      "16839/16839 [==============================] - 316s - loss: 2.6595   \n",
      "Epoch 10/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.6062Epoch 00009: loss improved from 2.65950 to 2.60606, saving model to model3-improvement-09-2.6061.hdf5\n",
      "16839/16839 [==============================] - 317s - loss: 2.6061   \n",
      "Epoch 11/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.5344Epoch 00010: loss improved from 2.60606 to 2.53479, saving model to model3-improvement-10-2.5348.hdf5\n",
      "16839/16839 [==============================] - 314s - loss: 2.5348   \n",
      "Epoch 12/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.4734Epoch 00011: loss improved from 2.53479 to 2.47271, saving model to model3-improvement-11-2.4727.hdf5\n",
      "16839/16839 [==============================] - 311s - loss: 2.4727   \n",
      "Epoch 13/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.4123Epoch 00012: loss improved from 2.47271 to 2.41344, saving model to model3-improvement-12-2.4134.hdf5\n",
      "16839/16839 [==============================] - 313s - loss: 2.4134   \n",
      "Epoch 14/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.3609Epoch 00013: loss improved from 2.41344 to 2.36102, saving model to model3-improvement-13-2.3610.hdf5\n",
      "16839/16839 [==============================] - 312s - loss: 2.3610   \n",
      "Epoch 15/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.3184Epoch 00014: loss improved from 2.36102 to 2.31813, saving model to model3-improvement-14-2.3181.hdf5\n",
      "16839/16839 [==============================] - 312s - loss: 2.3181   \n",
      "Epoch 16/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.2655Epoch 00015: loss improved from 2.31813 to 2.26598, saving model to model3-improvement-15-2.2660.hdf5\n",
      "16839/16839 [==============================] - 310s - loss: 2.2660   \n",
      "Epoch 17/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.2249Epoch 00016: loss improved from 2.26598 to 2.22427, saving model to model3-improvement-16-2.2243.hdf5\n",
      "16839/16839 [==============================] - 310s - loss: 2.2243   \n",
      "Epoch 18/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.1857Epoch 00017: loss improved from 2.22427 to 2.18548, saving model to model3-improvement-17-2.1855.hdf5\n",
      "16839/16839 [==============================] - 308s - loss: 2.1855   \n",
      "Epoch 19/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.1316Epoch 00018: loss improved from 2.18548 to 2.13218, saving model to model3-improvement-18-2.1322.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 2.1322   \n",
      "Epoch 20/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.0714Epoch 00019: loss improved from 2.13218 to 2.07152, saving model to model3-improvement-19-2.0715.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 2.0715   \n",
      "Epoch 21/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.0299Epoch 00020: loss improved from 2.07152 to 2.03049, saving model to model3-improvement-20-2.0305.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 2.0305   \n",
      "Epoch 22/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.9645Epoch 00021: loss improved from 2.03049 to 1.96430, saving model to model3-improvement-21-1.9643.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 1.9643   \n",
      "Epoch 23/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.8962Epoch 00022: loss improved from 1.96430 to 1.89656, saving model to model3-improvement-22-1.8966.hdf5\n",
      "16839/16839 [==============================] - 306s - loss: 1.8966   \n",
      "Epoch 24/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.8229Epoch 00023: loss improved from 1.89656 to 1.82238, saving model to model3-improvement-23-1.8224.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 1.8224   \n",
      "Epoch 25/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.7440Epoch 00024: loss improved from 1.82238 to 1.74444, saving model to model3-improvement-24-1.7444.hdf5\n",
      "16839/16839 [==============================] - 306s - loss: 1.7444   \n",
      "Epoch 26/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.6664Epoch 00025: loss improved from 1.74444 to 1.66606, saving model to model3-improvement-25-1.6661.hdf5\n",
      "16839/16839 [==============================] - 306s - loss: 1.6661   \n",
      "Epoch 27/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.5703Epoch 00026: loss improved from 1.66606 to 1.57001, saving model to model3-improvement-26-1.5700.hdf5\n",
      "16839/16839 [==============================] - 306s - loss: 1.5700   \n",
      "Epoch 28/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.4652Epoch 00027: loss improved from 1.57001 to 1.46604, saving model to model3-improvement-27-1.4660.hdf5\n",
      "16839/16839 [==============================] - 308s - loss: 1.4660   \n",
      "Epoch 29/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.3640Epoch 00028: loss improved from 1.46604 to 1.36459, saving model to model3-improvement-28-1.3646.hdf5\n",
      "16839/16839 [==============================] - 309s - loss: 1.3646   \n",
      "Epoch 30/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.2749Epoch 00029: loss improved from 1.36459 to 1.27612, saving model to model3-improvement-29-1.2761.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 1.2761   \n",
      "Epoch 31/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.1929Epoch 00030: loss improved from 1.27612 to 1.19297, saving model to model3-improvement-30-1.1930.hdf5\n",
      "16839/16839 [==============================] - 308s - loss: 1.1930   \n",
      "Epoch 32/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.0824Epoch 00031: loss improved from 1.19297 to 1.08219, saving model to model3-improvement-31-1.0822.hdf5\n",
      "16839/16839 [==============================] - 307s - loss: 1.0822   \n",
      "Epoch 33/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.9946Epoch 00032: loss improved from 1.08219 to 0.99483, saving model to model3-improvement-32-0.9948.hdf5\n",
      "16839/16839 [==============================] - 305s - loss: 0.9948   \n",
      "Epoch 34/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.9051Epoch 00033: loss improved from 0.99483 to 0.90585, saving model to model3-improvement-33-0.9059.hdf5\n",
      "16839/16839 [==============================] - 305s - loss: 0.9059   \n",
      "Epoch 35/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.8301Epoch 00034: loss improved from 0.90585 to 0.83032, saving model to model3-improvement-34-0.8303.hdf5\n",
      "16839/16839 [==============================] - 305s - loss: 0.8303   \n",
      "Epoch 36/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.7425Epoch 00035: loss improved from 0.83032 to 0.74246, saving model to model3-improvement-35-0.7425.hdf5\n",
      "16839/16839 [==============================] - 304s - loss: 0.7425   \n",
      "Epoch 37/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.6690Epoch 00036: loss improved from 0.74246 to 0.66907, saving model to model3-improvement-36-0.6691.hdf5\n",
      "16839/16839 [==============================] - 304s - loss: 0.6691   \n",
      "Epoch 38/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.6023Epoch 00037: loss improved from 0.66907 to 0.60285, saving model to model3-improvement-37-0.6029.hdf5\n",
      "16839/16839 [==============================] - 303s - loss: 0.6029   \n",
      "Epoch 39/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.5473Epoch 00038: loss improved from 0.60285 to 0.54830, saving model to model3-improvement-38-0.5483.hdf5\n",
      "16839/16839 [==============================] - 303s - loss: 0.5483   \n",
      "Epoch 40/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.5077Epoch 00039: loss improved from 0.54830 to 0.50712, saving model to model3-improvement-39-0.5071.hdf5\n",
      "16839/16839 [==============================] - 303s - loss: 0.5071   \n",
      "Epoch 41/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.4396Epoch 00040: loss improved from 0.50712 to 0.44014, saving model to model3-improvement-40-0.4401.hdf5\n",
      "16839/16839 [==============================] - 302s - loss: 0.4401   \n",
      "Epoch 42/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.3948Epoch 00041: loss improved from 0.44014 to 0.39465, saving model to model3-improvement-41-0.3947.hdf5\n",
      "16839/16839 [==============================] - 301s - loss: 0.3947   \n",
      "Epoch 43/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.3637Epoch 00042: loss improved from 0.39465 to 0.36365, saving model to model3-improvement-42-0.3637.hdf5\n",
      "16839/16839 [==============================] - 300s - loss: 0.3637   \n",
      "Epoch 44/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.3277Epoch 00043: loss improved from 0.36365 to 0.32748, saving model to model3-improvement-43-0.3275.hdf5\n",
      "16839/16839 [==============================] - 299s - loss: 0.3275   \n",
      "Epoch 45/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.3086Epoch 00044: loss improved from 0.32748 to 0.30868, saving model to model3-improvement-44-0.3087.hdf5\n",
      "16839/16839 [==============================] - 300s - loss: 0.3087   \n",
      "Epoch 46/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.2730Epoch 00045: loss improved from 0.30868 to 0.27383, saving model to model3-improvement-45-0.2738.hdf5\n",
      "16839/16839 [==============================] - 301s - loss: 0.2738   \n",
      "Epoch 47/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.2692Epoch 00046: loss improved from 0.27383 to 0.26917, saving model to model3-improvement-46-0.2692.hdf5\n",
      "16839/16839 [==============================] - 301s - loss: 0.2692   \n",
      "Epoch 48/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.2300Epoch 00047: loss improved from 0.26917 to 0.22989, saving model to model3-improvement-47-0.2299.hdf5\n",
      "16839/16839 [==============================] - 300s - loss: 0.2299   \n",
      "Epoch 49/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.2200Epoch 00048: loss improved from 0.22989 to 0.22008, saving model to model3-improvement-48-0.2201.hdf5\n",
      "16839/16839 [==============================] - 299s - loss: 0.2201   \n",
      "Epoch 50/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.2151Epoch 00049: loss improved from 0.22008 to 0.21561, saving model to model3-improvement-49-0.2156.hdf5\n",
      "16839/16839 [==============================] - 299s - loss: 0.2156   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efddf808b00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training time. I'm going to do fewer epochs b/c time constraint,\n",
    "# as well as slightly larger batch size for the same reason\n",
    "model3.fit(X, Y, epochs = 50, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se.\n",
      "  And sur the brave day sunk in hideous night,\n",
      "  When I behold the violet past prime,\n",
      "  And sable curls all silvered o'er with white:  \n",
      "  When lofty trees I see barren of leaves,\n",
      "  Which erst from heat did canopy the herd\n",
      "  And summer's green all girded up in sheaves\n",
      "  Borne on the bier with white and bristly beard:\n",
      "  Then of thy beauty do I question make\n",
      "  That thou among the wastes of time must go,\n",
      "  Since sweets and beauties do themselves forsake,\n",
      "  And die as fast as they see others grow\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the best epoch\n",
    "model3.load_weights('model3-improvement-49-0.2156.hdf5')\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random sequence from the data\n",
    "rand = np.random.randint(0, dataX2.shape[0]-1)\n",
    "seed = dataX2[rand]\n",
    "np.random.shuffle(seed)\n",
    "result = generate(model3, seed, vocab_len, int_to_char, length=500)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It worked a lot better with a larger network and more epochs. The loss was still improving as we iterated, so even more epochs could improve it further. The overfitting problem seems to still be an issue, as some of the generations appear to be straight from the data. There are still a few spelling errors here and there, but the network has learned most words, how to puncuate, and also even remembers to close parenthesis. I think this network would have performed even better with a larger dataset. Now I want to try and add another layer and increase dropout again to see if it improves. I also want to increase the learning rate, which will make the optimization algorithm update weights faster. This is a common suggestion when adding dropout to a networks training. \n",
    "\n",
    "I beleive that the best results from this network will come with a much larger text corpus, but due to time contraints, and the fact that this is the computer I use for other work, I am going to stick with this short dataset for now. I have been trying to get an AWS instance so I can train on the cloud, but they are not allowing me to use any instances with GPUs. Hopefully I can train this on a large dataset soon, and maybe my results will be near what Andrej Karpathy has shown with his network.\n",
    "\n",
    "Now I am going to play around with an even larger network. I tuned the model several times over a couple days and found the configuration below to work quite well. A few of my runs failed to give output due to a bug in keras. I attempted to up the learning rate in the optimization function, but it was failing to produce any output. Running it again without adjusting the learning rate seems to have fixed the problem, which leads me to believe there is a bug when adjusting the hyperparameters for the Adam optimizer. While I would love to play around with increasing the learning rate, I do not think I will be able to figure out how to fix this bug with the time I have for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 16889 number unique chars: 67\n",
      "unique chars:\n",
      "['r', 'd', 'p', 'v', '9', 'V', '4', 'A', ',', '2', 's', 'S', 'a', '0', 'G', 'u', 'U', ' ', 'b', 'j', 'C', 'q', 'Y', 'o', '\\n', '3', 'g', '?', 'O', 'R', 'x', 'k', 'I', ')', 'f', 'T', 'B', 'h', ':', '-', 'M', 't', 'z', 'l', '5', 'y', 'N', 'w', \"'\", 'c', 'n', 'P', ';', 'F', 'D', '7', '8', '6', 'e', 'L', '(', 'W', 'i', 'H', '.', '1', 'm']\n",
      "X shape: (16839, 50)\n",
      "Y shape: (16839,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data again, this time with a shorter sequence length\n",
    "dataX2,dataY2,char_to_int,int_to_char,vocab_len = prepare_data('sonnets.txt',verbose=True, seq_len=50)\n",
    "# Reshape X to follow keras.layers.LSTM specification\n",
    "X = np.reshape(dataX2, (dataX2.shape[0], dataX2.shape[1], 1))\n",
    "# Normalize X \n",
    "X = X/float(vocab_len)\n",
    "# Do the one-hot encoding of Y.\n",
    "Y = np_utils.to_categorical(dataY2)\n",
    "# Setup the 3 layer model\n",
    "model4 = Sequential()\n",
    "model4.add(LSTM(512, input_shape=(X.shape[1],X.shape[2]), return_sequences=True))\n",
    "model4.add(Dropout(0.4))\n",
    "model4.add(LSTM(512, return_sequences=True))\n",
    "model4.add(Dropout(0.4))\n",
    "model4.add(LSTM(512))\n",
    "model4.add(Dropout(0.4))\n",
    "model4.add(Dense(vocab_len, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "#Setup checkpoints\n",
    "file=\"model4-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 3.1018Epoch 00000: loss improved from inf to 3.10287, saving model to model4-improvement-00-3.1029.hdf5\n",
      "16839/16839 [==============================] - 467s - loss: 3.1029   \n",
      "Epoch 2/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 3.0240Epoch 00001: loss improved from 3.10287 to 3.02353, saving model to model4-improvement-01-3.0235.hdf5\n",
      "16839/16839 [==============================] - 459s - loss: 3.0235   \n",
      "Epoch 3/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 3.0066Epoch 00002: loss improved from 3.02353 to 3.00631, saving model to model4-improvement-02-3.0063.hdf5\n",
      "16839/16839 [==============================] - 456s - loss: 3.0063   \n",
      "Epoch 4/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.9747Epoch 00003: loss improved from 3.00631 to 2.97438, saving model to model4-improvement-03-2.9744.hdf5\n",
      "16839/16839 [==============================] - 457s - loss: 2.9744   \n",
      "Epoch 5/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.8649Epoch 00004: loss improved from 2.97438 to 2.86594, saving model to model4-improvement-04-2.8659.hdf5\n",
      "16839/16839 [==============================] - 454s - loss: 2.8659   \n",
      "Epoch 6/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.7969Epoch 00005: loss improved from 2.86594 to 2.79597, saving model to model4-improvement-05-2.7960.hdf5\n",
      "16839/16839 [==============================] - 454s - loss: 2.7960   \n",
      "Epoch 7/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.7115Epoch 00006: loss improved from 2.79597 to 2.71130, saving model to model4-improvement-06-2.7113.hdf5\n",
      "16839/16839 [==============================] - 455s - loss: 2.7113   \n",
      "Epoch 8/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.5949Epoch 00007: loss improved from 2.71130 to 2.59426, saving model to model4-improvement-07-2.5943.hdf5\n",
      "16839/16839 [==============================] - 453s - loss: 2.5943   \n",
      "Epoch 9/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.4930Epoch 00008: loss improved from 2.59426 to 2.49336, saving model to model4-improvement-08-2.4934.hdf5\n",
      "16839/16839 [==============================] - 453s - loss: 2.4934   \n",
      "Epoch 10/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.4159Epoch 00009: loss improved from 2.49336 to 2.41590, saving model to model4-improvement-09-2.4159.hdf5\n",
      "16839/16839 [==============================] - 452s - loss: 2.4159   \n",
      "Epoch 11/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.3683Epoch 00010: loss improved from 2.41590 to 2.36817, saving model to model4-improvement-10-2.3682.hdf5\n",
      "16839/16839 [==============================] - 456s - loss: 2.3682   \n",
      "Epoch 12/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.2957Epoch 00011: loss improved from 2.36817 to 2.29721, saving model to model4-improvement-11-2.2972.hdf5\n",
      "16839/16839 [==============================] - 466s - loss: 2.2972   \n",
      "Epoch 13/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.2451Epoch 00012: loss improved from 2.29721 to 2.24498, saving model to model4-improvement-12-2.2450.hdf5\n",
      "16839/16839 [==============================] - 469s - loss: 2.2450   \n",
      "Epoch 14/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.1831Epoch 00013: loss improved from 2.24498 to 2.18427, saving model to model4-improvement-13-2.1843.hdf5\n",
      "16839/16839 [==============================] - 460s - loss: 2.1843   \n",
      "Epoch 15/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.1513Epoch 00014: loss improved from 2.18427 to 2.14964, saving model to model4-improvement-14-2.1496.hdf5\n",
      "16839/16839 [==============================] - 459s - loss: 2.1496   \n",
      "Epoch 16/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.0952Epoch 00015: loss improved from 2.14964 to 2.09427, saving model to model4-improvement-15-2.0943.hdf5\n",
      "16839/16839 [==============================] - 460s - loss: 2.0943   \n",
      "Epoch 17/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 2.0298Epoch 00016: loss improved from 2.09427 to 2.03033, saving model to model4-improvement-16-2.0303.hdf5\n",
      "16839/16839 [==============================] - 462s - loss: 2.0303   \n",
      "Epoch 18/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.9681Epoch 00017: loss improved from 2.03033 to 1.96932, saving model to model4-improvement-17-1.9693.hdf5\n",
      "16839/16839 [==============================] - 463s - loss: 1.9693   \n",
      "Epoch 19/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.9016Epoch 00018: loss improved from 1.96932 to 1.90111, saving model to model4-improvement-18-1.9011.hdf5\n",
      "16839/16839 [==============================] - 464s - loss: 1.9011   \n",
      "Epoch 20/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.8307Epoch 00019: loss improved from 1.90111 to 1.83029, saving model to model4-improvement-19-1.8303.hdf5\n",
      "16839/16839 [==============================] - 468s - loss: 1.8303   \n",
      "Epoch 21/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.7620Epoch 00020: loss improved from 1.83029 to 1.76187, saving model to model4-improvement-20-1.7619.hdf5\n",
      "16839/16839 [==============================] - 473s - loss: 1.7619   \n",
      "Epoch 22/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 1.6647Epoch 00021: loss improved from 1.76187 to 1.66361, saving model to model4-improvement-21-1.6636.hdf5\n",
      "16839/16839 [==============================] - 479s - loss: 1.6636   \n",
      "Epoch 23/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 1.5616Epoch 00022: loss improved from 1.66361 to 1.56177, saving model to model4-improvement-22-1.5618.hdf5\n",
      "16839/16839 [==============================] - 481s - loss: 1.5618   \n",
      "Epoch 24/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.4901Epoch 00023: loss improved from 1.56177 to 1.48988, saving model to model4-improvement-23-1.4899.hdf5\n",
      "16839/16839 [==============================] - 471s - loss: 1.4899   \n",
      "Epoch 25/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.3715Epoch 00024: loss improved from 1.48988 to 1.37243, saving model to model4-improvement-24-1.3724.hdf5\n",
      "16839/16839 [==============================] - 468s - loss: 1.3724   \n",
      "Epoch 26/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.2803Epoch 00025: loss improved from 1.37243 to 1.28213, saving model to model4-improvement-25-1.2821.hdf5\n",
      "16839/16839 [==============================] - 469s - loss: 1.2821   \n",
      "Epoch 27/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.2034Epoch 00026: loss improved from 1.28213 to 1.20272, saving model to model4-improvement-26-1.2027.hdf5\n",
      "16839/16839 [==============================] - 469s - loss: 1.2027   \n",
      "Epoch 28/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.0892Epoch 00027: loss improved from 1.20272 to 1.08990, saving model to model4-improvement-27-1.0899.hdf5\n",
      "16839/16839 [==============================] - 472s - loss: 1.0899   \n",
      "Epoch 29/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 1.0094Epoch 00028: loss improved from 1.08990 to 1.00943, saving model to model4-improvement-28-1.0094.hdf5\n",
      "16839/16839 [==============================] - 473s - loss: 1.0094   \n",
      "Epoch 30/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.9121Epoch 00029: loss improved from 1.00943 to 0.91178, saving model to model4-improvement-29-0.9118.hdf5\n",
      "16839/16839 [==============================] - 478s - loss: 0.9118   \n",
      "Epoch 31/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.8138Epoch 00030: loss improved from 0.91178 to 0.81374, saving model to model4-improvement-30-0.8137.hdf5\n",
      "16839/16839 [==============================] - 480s - loss: 0.8137   \n",
      "Epoch 32/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.7520Epoch 00031: loss improved from 0.81374 to 0.75240, saving model to model4-improvement-31-0.7524.hdf5\n",
      "16839/16839 [==============================] - 487s - loss: 0.7524   \n",
      "Epoch 33/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.6746Epoch 00032: loss improved from 0.75240 to 0.67462, saving model to model4-improvement-32-0.6746.hdf5\n",
      "16839/16839 [==============================] - 494s - loss: 0.6746   \n",
      "Epoch 34/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.6026Epoch 00033: loss improved from 0.67462 to 0.60331, saving model to model4-improvement-33-0.6033.hdf5\n",
      "16839/16839 [==============================] - 485s - loss: 0.6033   \n",
      "Epoch 35/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.5396Epoch 00034: loss improved from 0.60331 to 0.53942, saving model to model4-improvement-34-0.5394.hdf5\n",
      "16839/16839 [==============================] - 474s - loss: 0.5394   \n",
      "Epoch 36/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.4970Epoch 00035: loss improved from 0.53942 to 0.49769, saving model to model4-improvement-35-0.4977.hdf5\n",
      "16839/16839 [==============================] - 466s - loss: 0.4977   \n",
      "Epoch 37/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.4725Epoch 00036: loss improved from 0.49769 to 0.47253, saving model to model4-improvement-36-0.4725.hdf5\n",
      "16839/16839 [==============================] - 465s - loss: 0.4725   \n",
      "Epoch 38/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.4265Epoch 00037: loss improved from 0.47253 to 0.42649, saving model to model4-improvement-37-0.4265.hdf5\n",
      "16839/16839 [==============================] - 464s - loss: 0.4265   \n",
      "Epoch 39/50\n",
      "16768/16839 [============================>.] - ETA: 1s - loss: 0.3987Epoch 00038: loss improved from 0.42649 to 0.39850, saving model to model4-improvement-38-0.3985.hdf5\n",
      "16839/16839 [==============================] - 464s - loss: 0.3985   \n",
      "Epoch 40/50\n",
      "16768/16839 [============================>.] - ETA: 2s - loss: 0.4046Epoch 00039: loss did not improve\n",
      "16839/16839 [==============================] - 479s - loss: 0.4042   \n",
      "Epoch 41/50\n",
      " 9856/16839 [================>.............] - ETA: 197s - loss: 0.3040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5789dbe550f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaacizy/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4.fit(X, Y, epochs = 50, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng wanth theu sear yous?t\n",
      "  And ivery fair wrom fair sometime declines,\n",
      "  By children's eyes, and child, and all wirm thiee,\n",
      "  Serking that beauteous roof to ruinate\n",
      "  Which this (Time's pencil) or my pupil \n",
      "  Wakes foth tuine eyes fnd allen fiter  Che lovely gaze where every eye doth dwell\n",
      "  Whiu lavpy sears the time that fair shoul arte.\n",
      "  How much more praise deserved thy beauty's use,\n",
      "  If thou couldst answer 'This fair child of mine\n",
      "  Shall sum my count, and make my old wirm\n",
      "  But thou cont\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the best epoch\n",
    "model4.load_weights('model4-improvement-38-0.3985.hdf5')\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random sequence from the data\n",
    "rand = np.random.randint(0, dataX2.shape[0]-1)\n",
    "seed = dataX2[rand]\n",
    "#np.random.shuffle(seed)\n",
    "result = generate(model4, seed, vocab_len, int_to_char, length=500)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the model is no longer overfitting the data, but it has many more spelling errors than before. I assume this is due to the small dataset I am using, in combination with the high dropout. I was hoping an increased learning rate would have helped with that, but due to the bug I am unable to test it at this time. \n",
    "\n",
    "I think this network configuration seems like its getting close enough to optimal that I would like to train it on a large dataset for a couple days, lowering the batch size a bit. But due to time constraints, this is all I will have trained before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Overall I am pretty satisfied with the results I was getting from such a small dataset. At first I was getting some serious overfitting problems, but increasing the number of layers and adding a higher dropout helped with that problem significantly, so much so that it may be affecting accuracy. An increase in learning rate may have helped with that issue, but I think the biggest thing that would have helped is a much larger dataset and more epochs. Unfortunately, I don't have the resources currently to train models like that in any reasonable time. My final network, even on the shortened dataset, still took over 16hrs for ~40 epochs, and that was for only one of the configurations I tried with the 3 layer network. \n",
    "\n",
    "I look forward to training this on a larger dataset once I can get some AWS instances or upgrade my computer. I would also like to explore different types of sequence based data with this same model like music and other audio recordings. Training this model on a collection of albums from a certain band would be a cool exercise that I am definitely going to be trying this summer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Jason Brownlee's blog post, a huge help with getting this up and running: \n",
    "\n",
    "http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "Original inspiration, some of the bots posts can be NSFW, so visit at your own discretion: \n",
    "\n",
    "http://reddit.com/r/subredditsimulator\n",
    "\n",
    "This blog post also helped push me towards this project (Andrej Karpathy):\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "Keras docs and source code:\n",
    "\n",
    "https://keras.io/\n",
    "https://github.com/fchollet/keras\n",
    "\n",
    "Paper on dropout and why it works for the overfitting problem:\n",
    "\n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
